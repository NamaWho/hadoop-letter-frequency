\newpage
\section{Introduction}
The objective of this project is to implement a data processing pipeline that can handle substantial data sets, ensuring efficient computation and meaningful data insights. By exploiting the MapReduce paradigm, data processing tasks is split into two main functions: the \textbf{Mapper} and the \textbf{Reducer}.\newline
The Mapper function processes the input data, emitting \textbf{key-value pairs}, while the Reducer aggregates these pairs to produce the final output.\\

\noindent The project is developed using Hadoop framework, which provides an open-source implementation of the MapReduce paradigm. Hadoop allows for the distributed processing of large data sets across clusters of computers using simple programming models. The Hadoop ecosystem also includes other tools, such as HDFS (Hadoop Distributed File System) for distributed storage, and YARN (Yet Another Resource Negotiator) for cluster resource management.\newline



