\documentclass[a4paper, 12pt]{article}

% Useful packages
\usepackage[utf8]{inputenc} % for handling accents
\usepackage{amsmath} % for advanced math
\usepackage{graphicx} % for handling images
\usepackage[hidelinks]{hyperref} % for hyperlinks
\usepackage{geometry} % for margin settings
\usepackage{xcolor} % for custom colors
\usepackage{titlesec} % for customizing section titles
\usepackage{titling} % for custom title, subtitle, and organization
\usepackage{algorithm} % for pseudocode
\usepackage{algpseudocode} % for pseudocode

% Margin settings
\geometry{a4paper, margin=1.5cm}

% Define custom colors
\definecolor{sectioncolor}{RGB}{0, 51, 102} % darker blue
\definecolor{subsectioncolor}{RGB}{0, 102, 102} % darker teal

% Customize section, subsection, and subsubsection without numbers
\titleformat{\section}
  {\color{sectioncolor}\large\bfseries} % Color and size for \section
  {}{0em}{} % Remove section number

\titleformat{\subsection}
  {\color{subsectioncolor}\normalsize\bfseries} % Color and size for \subsection
  {}{0em}{} % Remove subsection number

\titleformat{\subsubsection}
  {\color{subsectioncolor}\small\bfseries} % Color and size for \subsubsection
  {}{0em}{} % Remove subsubsection number



\begin{document}

\input{coverPage.tex}

\tableofcontents
\newpage
% Abstract
\begin{abstract}

  \noindent This report describes the project developed for Cloud Computing exam at the University of Pisa. It explores the algorithm design and the discussion about obtained results.

\end{abstract}


\section{Introduction}

The objective of this project is to implement a data processing pipeline that can handle substantial data sets, ensuring efficient computation and meaningful data insights. By exploiting the MapReduce paradigm, data processing tasks is split into two main functions: the Mapper and the Reducer. The Mapper function processes and filters the input data, emitting key-value pairs, while the Reducer function aggregates and processes these pairs to produce the final output.\\
The project was developed using the Hadoop framework, which provides an open-source implementation of the MapReduce paradigm. Hadoop allows for the distributed processing of large data sets across clusters of computers using simple programming models. The Hadoop ecosystem also includes other tools, such as HDFS (Hadoop Distributed File System) for distributed storage, and YARN (Yet Another Resource Negotiator) for cluster resource management.\\

\noindent Given a text document as input, it is aimed to extract the frequency of each letter composing such document. In Order to achieve this, two differnt jobs are required: the first job is responsible for counting the number of letters in the document, while the second job is responsible for evaluating the frequency of each letter.\\





\section{Algorithm Design}


\section{Results}


\section{Conclusions}



\end{document}
